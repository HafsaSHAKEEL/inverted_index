{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brUqsKSqHscs",
        "outputId": "c6bd74b1-f907-488c-b148-043d2257f25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 1**"
      ],
      "metadata": {
        "id": "RFnEMDrSR4o6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRINTING THE CONTENTS OF ALL FILES**"
      ],
      "metadata": {
        "id": "n0LuQqtdSNdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = ['1.txt', '2.txt', '3.txt', '4.txt', '5.txt']"
      ],
      "metadata": {
        "id": "7kFwH7OIROBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = []"
      ],
      "metadata": {
        "id": "1rL5nak9RTZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in file_names:\n",
        "    with open(file_name, 'r') as file:\n",
        "        file_contents.append(file.read())"
      ],
      "metadata": {
        "id": "BRa_MF6mRZj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the contents of each file\n",
        "\n",
        "for i, content in enumerate(file_contents):\n",
        "    print(f\"Contents of file {i+1}:\")\n",
        "    print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pnLae2fRgqK",
        "outputId": "aa52a9a7-75d3-449a-c0c7-f2bf64f40a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of file 1:\n",
            "ملک امن امان صورت حال کنٹرول فواد چوہدری اسلام وزیراطلاعات فواد چوہدری کہنا ملک امن امان صورت حال کنٹرول ہے، ریاست صبر کمزوری سمجھا جائے۔ خیالات اظہار سماجی رابطے ویب سائٹ ٹوئٹر جاری کردہ بیان کیا، فواد چوہدری کہنا آئین قانون مکمل عملداری عوام مفاد ہے۔ فرض پورا گے، وزیر اعظم صورت حال لمحہ بہ لمحہ باخبر جارہا ہے۔ مزید کہنا چین اہم دورے سبوتاژ کوشش ناکام ہوگی۔ خوش گمانی ریاست کمزورہے، اپوزیشن ادارے حکومت ساتھ ہیں، فواد چوہدری گذشتہ روز وزیراطلاعات فواد چوہدری اپوزیشن مذاکرات کررہے ہیں، حکومت حکمت عملی اپوزیشن آگاہ ہے، اہم ایشو پورا پاکستان متحد ہے۔ حکومت ساتھ چلنا چاہتی ہے، حکومت جانب آئندہ لائحہ عممل تیار جارہا ہے۔\n",
            "Contents of file 2:\n",
            "کم جونگ جلد جنوبی کوریا دورہ پیانگ شمالی کوریا رہنما کم جونگ بار جنوبی کوریا دورہ دوران اہم ملاقاتیں گی۔ تفصیلات مطابق جنوبی کوریا حکام کہنا کم جونگ جلد ملک دورہ گے، صدر مون جے علاوہ دیگر اعلیٰ عہدیداروں ملاقاتیں گی۔ غیر ملکی خبر رساں ادارے مطابق جنوبی کوریا صدر مون جے کہنا شمالی کوریائی رہنما کم جونگ جلد سیول دورہ گے۔ دورے خطے جوہری ہتھیاروں پاک بنانے حوالے گفتگو گی، باہمی تعلقات مزید مضبوط بنانے زور گا۔ یاد مون جے کم جونگ تاریخی ملاقات رواں سال اپریل تھی، صدر شمالی کوریا خصوصی ملاقات جنوبی کوریا پہنچے تھے۔ ازاں رہنماؤں مشترکہ پریس کانفرنس باہمی جنگ خاتمے اعلان ساتھ ساتھ جوہری ہتھیاروں عدم پھیلاؤ اتفاق تھا۔ کم جونگ جنوبی کوریا آمد، پرتباک استقبال، تاریخی ملاقات خیال ممالک گذشتہ کئی سالوں دوسرے حریف ہیں، ماضی دوسرے ملکوں تباہ سنگین دھمکیاں ہیں۔ واضح شمالی کوریا جانب اعلان جوہری پروگرام ترک کردیا ہے، ازاں حکام بین الاقوامی معائنہ کاروں جوہری تجربہ گاہ معائنے اجازت ہے۔\n",
            "Contents of file 3:\n",
            "چینی صدر امریکی منصب ٹیلی فونک رابطہ، تجارتی معاملات گفتگو چینی صدر شی پنگ امریکی منصب ڈونلڈ ٹرمپ ٹیلی فونک رابطہ ہوا، دوران تجارتی معاملات تبادلہ خیال گیا۔ تفصیلات مطابق ٹیلی فونک گفتگو دوران ملکوں صدور تجارتی معاملات علاوہ شمالی کوریا موضوع گفتگو کی۔ غیر ملکی خبر رساں ادارے مطابق چینی صدر شی پنگ کہنا چین خطے ہمیشہ امن استحکام بات ہے، عالمی سطح قیام امن کوششیں جاری رکھیں گے۔ امریکی صدر گفتگو دوران چینی صدر مزید کہنا جزیرہ نما کوریا جوہری ہتھیاروں صاف بنانے بھرپور مدد فراہم گی۔ دوسری جانب امریکی صدر ڈونلڈ ٹرمپ جاری کردہ بیان واضح چینی صدر شی پنگ ساتھ بات چیت اچھی رہی۔ چین امریکا تعلقات کشیدگی ختم اہم اقدامات ملکوں سربراہان جلد ملاقات اتفاق ہے۔ واضح حالیہ چند ہفتوں چین امریکا درمیان شدید تجارتی جنگ جاری ہے۔ خیال رواں سال جون امریکا پہلی مرتبہ چینی مصنوعات چونتیس بلین ڈالر اضافی محصولات عائد فوری ردعمل چین امریکی مصنوعات ساٹھ بلین ڈالر اضافی ٹیکس عائد اعلان تھا۔ یاد رواں سال اگست امریکی حکام جانب چینی درآمدات مزید ارب ڈالر اضافی ٹیکس عائد تھے۔\n",
            "Contents of file 4:\n",
            "فاٹا خیبر پختونخوا انضمام اکثریت فیصلہ ہے، وزیرِ اطلاعات شوکت یوسف زئی صوبائی وزیرِ اطلاعات شوکت یوسف زئی فاٹا خیبر پختونخوا انضمام اکثریت فیصلہ ہے۔ صوبائی وزیرِ اطلاعات مٹھی بھر عناصر بیرونی مفادات فاٹا انضمام مخالفت ہیں، پی فاٹا انضمام اکثریت فیصلہ ہے۔ قبائلی روایات مطابق ڈی آر سیز دائرہ کار فاٹا پھیلائیں وزیرِ اطلاعات شوکت یوسف زئی کہنا حکومت سنجیدہ ہے، فاٹا اصلاحات تیزی حکومت سنجیدہ اقدامات ہے۔ وزیرِ اطلاعات قبائلی روایات مطابق ڈی آر سیز دائرہ کار فاٹا پھیلائیں گے، قبائلی علاقوں صحت تعلیم معیار بہتر بنا ہیں۔ صوبائی وزیر شوکت یوسف زئی کہنا قبائلی عوام فوری انصاف فراہمی حکومتی ترجیحات سرِ فہرست ہے۔ یاد ماہ قبل گورنر خیبر پختونخوا سمیت فاٹا ارکانِ قومی اسمبلی وزیرِ اعظم عمران خان مطالبہ فاٹا قومی مالیاتی کمیشن ایف ایوارڈ تین فی صد فوری طور ادا جائے۔ این ایف ایوارڈ فی صد حصہ جلد از جلد فاٹا جائے، فاٹا ارکان وزیرِ اعظم عمران خان مطالبہ ارکان عمران خان مطالبہ فاٹا علاقوں ایم پی ایز تعداد بڑھا جائے، فاٹا ایم این ایز موجودہ تعداد برقرار جائے۔ فاٹا ارکان وزیرِ اعظم عمران خان مطالبہ ڈی پی ترقیاتی مختص رقم جلد از جلد جاری جائے۔\n",
            "Contents of file 5:\n",
            "پوری کوشش معاملے افہام تفہیم حل کریں، وزیرِ مملکت برائے داخلہ اسلام وزیرِ مملکت برائے داخلہ شہریار آفریدی سپریم کورٹ فیصلے ملک پیدا صورتِ حال پوری کوشش معاملے افہام تفہیم حل کریں۔ شہریار آفریدی آر وائی نیوز گفتگو آج رات معاملات حل ہوجائیں گے، قانون ہاتھ لینے لوگ گرفتار ہیں، پوری کوشش معاملے افہام تفہیم حل کریں۔ موبائل سروس بند ش متعلق معاملے علم شہریار آفریدی وزیر مملکت موبائل سروس بند ش متعلق معاملے علم نہیں، دستخط بغیر اقدام گا، ریاستی امور وقت بات مناسب نہیں۔ انھوں دھرنے قیادت رابطہ ہے، بات چیت ہے، دھرنے قیادت کمیٹی مطالبات ہیں، عوام ملک بہتر ہوگا ریاست کردار ادا گی۔ کشیدہ صورت حال، ملک مختلف شہروں اسکول بند اعلان شہریار آفریدی کہنا پاکستان ادارے معاملات سمجھوتہ گے، وزیرِ اعظم واضح پیغام ریاست ڈکٹیٹ سکتا، آسیہ بی بی نام ای ایل بات آئی۔ وزیرِ مملکت اقلیتی برادری اتنی عزت جتنے دیگر شہریوں ہے، عمران خان وزیرِ اعظم بننے یورپ سوچ بدلی ہے، حکومت ہالینڈ گستاخانہ خاکوں معاملے خلاف آواز اٹھائی۔\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REMOVING NON-URDU WORDS**"
      ],
      "metadata": {
        "id": "_DdvSOnaSfHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re #For regex pattern\n",
        "import os"
      ],
      "metadata": {
        "id": "JQscOGZoSjhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_urdu_word(word):\n",
        "    # define a regex pattern to match Urdu characters\n",
        "    pattern = re.compile(\"^[\\u0600-\\u06FF]+$\")\n",
        "    return pattern.match(word) is not None"
      ],
      "metadata": {
        "id": "Tzjlb2-SSmpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_urdu_words(filename):\n",
        "    #Reading the contents of all the files\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "\n",
        "    #Spliting the string into words\n",
        "    words = contents.split()\n",
        "\n",
        "    #Filtering out the non-Urdu words\n",
        "    urdu_words = [word for word in words if is_urdu_word(word)]\n",
        "\n",
        "    #Joining the filtered words back into a string\n",
        "    filtered_contents = \" \".join(urdu_words)\n",
        "\n",
        "    print(filtered_contents)\n",
        "\n",
        "    #writing the filtered string back to the file\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(filtered_contents)"
      ],
      "metadata": {
        "id": "7olK5caMTLYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\"1.txt\", \"2.txt\", \"3.txt\", \"4.txt\", \"5.txt\"]\n",
        "\n",
        "#process each of the files\n",
        "for filename in filenames:\n",
        "    if os.path.exists(filename):\n",
        "        remove_non_urdu_words(filename)\n",
        "    else:\n",
        "        print(f\"{filename} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxIPGFXZTnya",
        "outputId": "debbf390-d7b9-4265-e189-ee3aeb60c952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ملک امن امان صورت حال کنٹرول فواد چوہدری اسلام وزیراطلاعات فواد چوہدری کہنا ملک امن امان صورت حال کنٹرول ہے، ریاست صبر کمزوری سمجھا جائے۔ خیالات اظہار سماجی رابطے ویب سائٹ ٹوئٹر جاری کردہ بیان کیا، فواد چوہدری کہنا آئین قانون مکمل عملداری عوام مفاد ہے۔ فرض پورا گے، وزیر اعظم صورت حال لمحہ بہ لمحہ باخبر جارہا ہے۔ مزید کہنا چین اہم دورے سبوتاژ کوشش ناکام ہوگی۔ خوش گمانی ریاست کمزورہے، اپوزیشن ادارے حکومت ساتھ ہیں، فواد چوہدری گذشتہ روز وزیراطلاعات فواد چوہدری اپوزیشن مذاکرات کررہے ہیں، حکومت حکمت عملی اپوزیشن آگاہ ہے، اہم ایشو پورا پاکستان متحد ہے۔ حکومت ساتھ چلنا چاہتی ہے، حکومت جانب آئندہ لائحہ عممل تیار جارہا ہے۔\n",
            "کم جونگ جلد جنوبی کوریا دورہ پیانگ شمالی کوریا رہنما کم جونگ بار جنوبی کوریا دورہ دوران اہم ملاقاتیں گی۔ تفصیلات مطابق جنوبی کوریا حکام کہنا کم جونگ جلد ملک دورہ گے، صدر مون جے علاوہ دیگر اعلیٰ عہدیداروں ملاقاتیں گی۔ غیر ملکی خبر رساں ادارے مطابق جنوبی کوریا صدر مون جے کہنا شمالی کوریائی رہنما کم جونگ جلد سیول دورہ گے۔ دورے خطے جوہری ہتھیاروں پاک بنانے حوالے گفتگو گی، باہمی تعلقات مزید مضبوط بنانے زور گا۔ یاد مون جے کم جونگ تاریخی ملاقات رواں سال اپریل تھی، صدر شمالی کوریا خصوصی ملاقات جنوبی کوریا پہنچے تھے۔ ازاں رہنماؤں مشترکہ پریس کانفرنس باہمی جنگ خاتمے اعلان ساتھ ساتھ جوہری ہتھیاروں عدم پھیلاؤ اتفاق تھا۔ کم جونگ جنوبی کوریا آمد، پرتباک استقبال، تاریخی ملاقات خیال ممالک گذشتہ کئی سالوں دوسرے حریف ہیں، ماضی دوسرے ملکوں تباہ سنگین دھمکیاں ہیں۔ واضح شمالی کوریا جانب اعلان جوہری پروگرام ترک کردیا ہے، ازاں حکام بین الاقوامی معائنہ کاروں جوہری تجربہ گاہ معائنے اجازت ہے۔\n",
            "چینی صدر امریکی منصب ٹیلی فونک رابطہ، تجارتی معاملات گفتگو چینی صدر شی پنگ امریکی منصب ڈونلڈ ٹرمپ ٹیلی فونک رابطہ ہوا، دوران تجارتی معاملات تبادلہ خیال گیا۔ تفصیلات مطابق ٹیلی فونک گفتگو دوران ملکوں صدور تجارتی معاملات علاوہ شمالی کوریا موضوع گفتگو کی۔ غیر ملکی خبر رساں ادارے مطابق چینی صدر شی پنگ کہنا چین خطے ہمیشہ امن استحکام بات ہے، عالمی سطح قیام امن کوششیں جاری رکھیں گے۔ امریکی صدر گفتگو دوران چینی صدر مزید کہنا جزیرہ نما کوریا جوہری ہتھیاروں صاف بنانے بھرپور مدد فراہم گی۔ دوسری جانب امریکی صدر ڈونلڈ ٹرمپ جاری کردہ بیان واضح چینی صدر شی پنگ ساتھ بات چیت اچھی رہی۔ چین امریکا تعلقات کشیدگی ختم اہم اقدامات ملکوں سربراہان جلد ملاقات اتفاق ہے۔ واضح حالیہ چند ہفتوں چین امریکا درمیان شدید تجارتی جنگ جاری ہے۔ خیال رواں سال جون امریکا پہلی مرتبہ چینی مصنوعات چونتیس بلین ڈالر اضافی محصولات عائد فوری ردعمل چین امریکی مصنوعات ساٹھ بلین ڈالر اضافی ٹیکس عائد اعلان تھا۔ یاد رواں سال اگست امریکی حکام جانب چینی درآمدات مزید ارب ڈالر اضافی ٹیکس عائد تھے۔\n",
            "فاٹا خیبر پختونخوا انضمام اکثریت فیصلہ ہے، وزیرِ اطلاعات شوکت یوسف زئی صوبائی وزیرِ اطلاعات شوکت یوسف زئی فاٹا خیبر پختونخوا انضمام اکثریت فیصلہ ہے۔ صوبائی وزیرِ اطلاعات مٹھی بھر عناصر بیرونی مفادات فاٹا انضمام مخالفت ہیں، پی فاٹا انضمام اکثریت فیصلہ ہے۔ قبائلی روایات مطابق ڈی آر سیز دائرہ کار فاٹا پھیلائیں وزیرِ اطلاعات شوکت یوسف زئی کہنا حکومت سنجیدہ ہے، فاٹا اصلاحات تیزی حکومت سنجیدہ اقدامات ہے۔ وزیرِ اطلاعات قبائلی روایات مطابق ڈی آر سیز دائرہ کار فاٹا پھیلائیں گے، قبائلی علاقوں صحت تعلیم معیار بہتر بنا ہیں۔ صوبائی وزیر شوکت یوسف زئی کہنا قبائلی عوام فوری انصاف فراہمی حکومتی ترجیحات سرِ فہرست ہے۔ یاد ماہ قبل گورنر خیبر پختونخوا سمیت فاٹا ارکانِ قومی اسمبلی وزیرِ اعظم عمران خان مطالبہ فاٹا قومی مالیاتی کمیشن ایف ایوارڈ تین فی صد فوری طور ادا جائے۔ این ایف ایوارڈ فی صد حصہ جلد از جلد فاٹا جائے، فاٹا ارکان وزیرِ اعظم عمران خان مطالبہ ارکان عمران خان مطالبہ فاٹا علاقوں ایم پی ایز تعداد بڑھا جائے، فاٹا ایم این ایز موجودہ تعداد برقرار جائے۔ فاٹا ارکان وزیرِ اعظم عمران خان مطالبہ ڈی پی ترقیاتی مختص رقم جلد از جلد جاری جائے۔\n",
            "پوری کوشش معاملے افہام تفہیم حل کریں، وزیرِ مملکت برائے داخلہ اسلام وزیرِ مملکت برائے داخلہ شہریار آفریدی سپریم کورٹ فیصلے ملک پیدا صورتِ حال پوری کوشش معاملے افہام تفہیم حل کریں۔ شہریار آفریدی آر وائی نیوز گفتگو آج رات معاملات حل ہوجائیں گے، قانون ہاتھ لینے لوگ گرفتار ہیں، پوری کوشش معاملے افہام تفہیم حل کریں۔ موبائل سروس بند ش متعلق معاملے علم شہریار آفریدی وزیر مملکت موبائل سروس بند ش متعلق معاملے علم نہیں، دستخط بغیر اقدام گا، ریاستی امور وقت بات مناسب نہیں۔ انھوں دھرنے قیادت رابطہ ہے، بات چیت ہے، دھرنے قیادت کمیٹی مطالبات ہیں، عوام ملک بہتر ہوگا ریاست کردار ادا گی۔ کشیدہ صورت حال، ملک مختلف شہروں اسکول بند اعلان شہریار آفریدی کہنا پاکستان ادارے معاملات سمجھوتہ گے، وزیرِ اعظم واضح پیغام ریاست ڈکٹیٹ سکتا، آسیہ بی بی نام ای ایل بات آئی۔ وزیرِ مملکت اقلیتی برادری اتنی عزت جتنے دیگر شہریوں ہے، عمران خان وزیرِ اعظم بننے یورپ سوچ بدلی ہے، حکومت ہالینڈ گستاخانہ خاکوں معاملے خلاف آواز اٹھائی۔\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION**"
      ],
      "metadata": {
        "id": "3TYFFDR5UemK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_urdu_words(filename):\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "\n",
        "    words = contents.split()\n",
        "    urdu_words = [word for word in words if is_urdu_word(word)]\n",
        "    filtered_contents = \" \".join(urdu_words)\n",
        "\n",
        "    #tokenizing the filtered string into a list of tokens\n",
        "    tokens = filtered_contents.split()\n",
        "    print(tokens)\n",
        "\n",
        "    #write the filtered string back to the file\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(filtered_contents)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "8HhGx0qaUoCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process each of the files\n",
        "all_tokens = []\n",
        "for filename in filenames:\n",
        "    if os.path.exists(filename):\n",
        "        tokens = remove_non_urdu_words(filename)\n",
        "        all_tokens.extend(tokens)\n",
        "    else:\n",
        "        print(f\"{filename} does not exist.\")\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8pTtiVtBqKV",
        "outputId": "c1b4da18-e567-4adb-a5d9-1d30de1dfd82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ملک', 'امن', 'امان', 'صورت', 'حال', 'کنٹرول', 'فواد', 'چوہدری', 'اسلام', 'وزیراطلاعات', 'فواد', 'چوہدری', 'کہنا', 'ملک', 'امن', 'امان', 'صورت', 'حال', 'کنٹرول', 'ہے،', 'ریاست', 'صبر', 'کمزوری', 'سمجھا', 'جائے۔', 'خیالات', 'اظہار', 'سماجی', 'رابطے', 'ویب', 'سائٹ', 'ٹوئٹر', 'جاری', 'کردہ', 'بیان', 'کیا،', 'فواد', 'چوہدری', 'کہنا', 'آئین', 'قانون', 'مکمل', 'عملداری', 'عوام', 'مفاد', 'ہے۔', 'فرض', 'پورا', 'گے،', 'وزیر', 'اعظم', 'صورت', 'حال', 'لمحہ', 'بہ', 'لمحہ', 'باخبر', 'جارہا', 'ہے۔', 'مزید', 'کہنا', 'چین', 'اہم', 'دورے', 'سبوتاژ', 'کوشش', 'ناکام', 'ہوگی۔', 'خوش', 'گمانی', 'ریاست', 'کمزورہے،', 'اپوزیشن', 'ادارے', 'حکومت', 'ساتھ', 'ہیں،', 'فواد', 'چوہدری', 'گذشتہ', 'روز', 'وزیراطلاعات', 'فواد', 'چوہدری', 'اپوزیشن', 'مذاکرات', 'کررہے', 'ہیں،', 'حکومت', 'حکمت', 'عملی', 'اپوزیشن', 'آگاہ', 'ہے،', 'اہم', 'ایشو', 'پورا', 'پاکستان', 'متحد', 'ہے۔', 'حکومت', 'ساتھ', 'چلنا', 'چاہتی', 'ہے،', 'حکومت', 'جانب', 'آئندہ', 'لائحہ', 'عممل', 'تیار', 'جارہا', 'ہے۔']\n",
            "['کم', 'جونگ', 'جلد', 'جنوبی', 'کوریا', 'دورہ', 'پیانگ', 'شمالی', 'کوریا', 'رہنما', 'کم', 'جونگ', 'بار', 'جنوبی', 'کوریا', 'دورہ', 'دوران', 'اہم', 'ملاقاتیں', 'گی۔', 'تفصیلات', 'مطابق', 'جنوبی', 'کوریا', 'حکام', 'کہنا', 'کم', 'جونگ', 'جلد', 'ملک', 'دورہ', 'گے،', 'صدر', 'مون', 'جے', 'علاوہ', 'دیگر', 'اعلیٰ', 'عہدیداروں', 'ملاقاتیں', 'گی۔', 'غیر', 'ملکی', 'خبر', 'رساں', 'ادارے', 'مطابق', 'جنوبی', 'کوریا', 'صدر', 'مون', 'جے', 'کہنا', 'شمالی', 'کوریائی', 'رہنما', 'کم', 'جونگ', 'جلد', 'سیول', 'دورہ', 'گے۔', 'دورے', 'خطے', 'جوہری', 'ہتھیاروں', 'پاک', 'بنانے', 'حوالے', 'گفتگو', 'گی،', 'باہمی', 'تعلقات', 'مزید', 'مضبوط', 'بنانے', 'زور', 'گا۔', 'یاد', 'مون', 'جے', 'کم', 'جونگ', 'تاریخی', 'ملاقات', 'رواں', 'سال', 'اپریل', 'تھی،', 'صدر', 'شمالی', 'کوریا', 'خصوصی', 'ملاقات', 'جنوبی', 'کوریا', 'پہنچے', 'تھے۔', 'ازاں', 'رہنماؤں', 'مشترکہ', 'پریس', 'کانفرنس', 'باہمی', 'جنگ', 'خاتمے', 'اعلان', 'ساتھ', 'ساتھ', 'جوہری', 'ہتھیاروں', 'عدم', 'پھیلاؤ', 'اتفاق', 'تھا۔', 'کم', 'جونگ', 'جنوبی', 'کوریا', 'آمد،', 'پرتباک', 'استقبال،', 'تاریخی', 'ملاقات', 'خیال', 'ممالک', 'گذشتہ', 'کئی', 'سالوں', 'دوسرے', 'حریف', 'ہیں،', 'ماضی', 'دوسرے', 'ملکوں', 'تباہ', 'سنگین', 'دھمکیاں', 'ہیں۔', 'واضح', 'شمالی', 'کوریا', 'جانب', 'اعلان', 'جوہری', 'پروگرام', 'ترک', 'کردیا', 'ہے،', 'ازاں', 'حکام', 'بین', 'الاقوامی', 'معائنہ', 'کاروں', 'جوہری', 'تجربہ', 'گاہ', 'معائنے', 'اجازت', 'ہے۔']\n",
            "['چینی', 'صدر', 'امریکی', 'منصب', 'ٹیلی', 'فونک', 'رابطہ،', 'تجارتی', 'معاملات', 'گفتگو', 'چینی', 'صدر', 'شی', 'پنگ', 'امریکی', 'منصب', 'ڈونلڈ', 'ٹرمپ', 'ٹیلی', 'فونک', 'رابطہ', 'ہوا،', 'دوران', 'تجارتی', 'معاملات', 'تبادلہ', 'خیال', 'گیا۔', 'تفصیلات', 'مطابق', 'ٹیلی', 'فونک', 'گفتگو', 'دوران', 'ملکوں', 'صدور', 'تجارتی', 'معاملات', 'علاوہ', 'شمالی', 'کوریا', 'موضوع', 'گفتگو', 'کی۔', 'غیر', 'ملکی', 'خبر', 'رساں', 'ادارے', 'مطابق', 'چینی', 'صدر', 'شی', 'پنگ', 'کہنا', 'چین', 'خطے', 'ہمیشہ', 'امن', 'استحکام', 'بات', 'ہے،', 'عالمی', 'سطح', 'قیام', 'امن', 'کوششیں', 'جاری', 'رکھیں', 'گے۔', 'امریکی', 'صدر', 'گفتگو', 'دوران', 'چینی', 'صدر', 'مزید', 'کہنا', 'جزیرہ', 'نما', 'کوریا', 'جوہری', 'ہتھیاروں', 'صاف', 'بنانے', 'بھرپور', 'مدد', 'فراہم', 'گی۔', 'دوسری', 'جانب', 'امریکی', 'صدر', 'ڈونلڈ', 'ٹرمپ', 'جاری', 'کردہ', 'بیان', 'واضح', 'چینی', 'صدر', 'شی', 'پنگ', 'ساتھ', 'بات', 'چیت', 'اچھی', 'رہی۔', 'چین', 'امریکا', 'تعلقات', 'کشیدگی', 'ختم', 'اہم', 'اقدامات', 'ملکوں', 'سربراہان', 'جلد', 'ملاقات', 'اتفاق', 'ہے۔', 'واضح', 'حالیہ', 'چند', 'ہفتوں', 'چین', 'امریکا', 'درمیان', 'شدید', 'تجارتی', 'جنگ', 'جاری', 'ہے۔', 'خیال', 'رواں', 'سال', 'جون', 'امریکا', 'پہلی', 'مرتبہ', 'چینی', 'مصنوعات', 'چونتیس', 'بلین', 'ڈالر', 'اضافی', 'محصولات', 'عائد', 'فوری', 'ردعمل', 'چین', 'امریکی', 'مصنوعات', 'ساٹھ', 'بلین', 'ڈالر', 'اضافی', 'ٹیکس', 'عائد', 'اعلان', 'تھا۔', 'یاد', 'رواں', 'سال', 'اگست', 'امریکی', 'حکام', 'جانب', 'چینی', 'درآمدات', 'مزید', 'ارب', 'ڈالر', 'اضافی', 'ٹیکس', 'عائد', 'تھے۔']\n",
            "['فاٹا', 'خیبر', 'پختونخوا', 'انضمام', 'اکثریت', 'فیصلہ', 'ہے،', 'وزیرِ', 'اطلاعات', 'شوکت', 'یوسف', 'زئی', 'صوبائی', 'وزیرِ', 'اطلاعات', 'شوکت', 'یوسف', 'زئی', 'فاٹا', 'خیبر', 'پختونخوا', 'انضمام', 'اکثریت', 'فیصلہ', 'ہے۔', 'صوبائی', 'وزیرِ', 'اطلاعات', 'مٹھی', 'بھر', 'عناصر', 'بیرونی', 'مفادات', 'فاٹا', 'انضمام', 'مخالفت', 'ہیں،', 'پی', 'فاٹا', 'انضمام', 'اکثریت', 'فیصلہ', 'ہے۔', 'قبائلی', 'روایات', 'مطابق', 'ڈی', 'آر', 'سیز', 'دائرہ', 'کار', 'فاٹا', 'پھیلائیں', 'وزیرِ', 'اطلاعات', 'شوکت', 'یوسف', 'زئی', 'کہنا', 'حکومت', 'سنجیدہ', 'ہے،', 'فاٹا', 'اصلاحات', 'تیزی', 'حکومت', 'سنجیدہ', 'اقدامات', 'ہے۔', 'وزیرِ', 'اطلاعات', 'قبائلی', 'روایات', 'مطابق', 'ڈی', 'آر', 'سیز', 'دائرہ', 'کار', 'فاٹا', 'پھیلائیں', 'گے،', 'قبائلی', 'علاقوں', 'صحت', 'تعلیم', 'معیار', 'بہتر', 'بنا', 'ہیں۔', 'صوبائی', 'وزیر', 'شوکت', 'یوسف', 'زئی', 'کہنا', 'قبائلی', 'عوام', 'فوری', 'انصاف', 'فراہمی', 'حکومتی', 'ترجیحات', 'سرِ', 'فہرست', 'ہے۔', 'یاد', 'ماہ', 'قبل', 'گورنر', 'خیبر', 'پختونخوا', 'سمیت', 'فاٹا', 'ارکانِ', 'قومی', 'اسمبلی', 'وزیرِ', 'اعظم', 'عمران', 'خان', 'مطالبہ', 'فاٹا', 'قومی', 'مالیاتی', 'کمیشن', 'ایف', 'ایوارڈ', 'تین', 'فی', 'صد', 'فوری', 'طور', 'ادا', 'جائے۔', 'این', 'ایف', 'ایوارڈ', 'فی', 'صد', 'حصہ', 'جلد', 'از', 'جلد', 'فاٹا', 'جائے،', 'فاٹا', 'ارکان', 'وزیرِ', 'اعظم', 'عمران', 'خان', 'مطالبہ', 'ارکان', 'عمران', 'خان', 'مطالبہ', 'فاٹا', 'علاقوں', 'ایم', 'پی', 'ایز', 'تعداد', 'بڑھا', 'جائے،', 'فاٹا', 'ایم', 'این', 'ایز', 'موجودہ', 'تعداد', 'برقرار', 'جائے۔', 'فاٹا', 'ارکان', 'وزیرِ', 'اعظم', 'عمران', 'خان', 'مطالبہ', 'ڈی', 'پی', 'ترقیاتی', 'مختص', 'رقم', 'جلد', 'از', 'جلد', 'جاری', 'جائے۔']\n",
            "['پوری', 'کوشش', 'معاملے', 'افہام', 'تفہیم', 'حل', 'کریں،', 'وزیرِ', 'مملکت', 'برائے', 'داخلہ', 'اسلام', 'وزیرِ', 'مملکت', 'برائے', 'داخلہ', 'شہریار', 'آفریدی', 'سپریم', 'کورٹ', 'فیصلے', 'ملک', 'پیدا', 'صورتِ', 'حال', 'پوری', 'کوشش', 'معاملے', 'افہام', 'تفہیم', 'حل', 'کریں۔', 'شہریار', 'آفریدی', 'آر', 'وائی', 'نیوز', 'گفتگو', 'آج', 'رات', 'معاملات', 'حل', 'ہوجائیں', 'گے،', 'قانون', 'ہاتھ', 'لینے', 'لوگ', 'گرفتار', 'ہیں،', 'پوری', 'کوشش', 'معاملے', 'افہام', 'تفہیم', 'حل', 'کریں۔', 'موبائل', 'سروس', 'بند', 'ش', 'متعلق', 'معاملے', 'علم', 'شہریار', 'آفریدی', 'وزیر', 'مملکت', 'موبائل', 'سروس', 'بند', 'ش', 'متعلق', 'معاملے', 'علم', 'نہیں،', 'دستخط', 'بغیر', 'اقدام', 'گا،', 'ریاستی', 'امور', 'وقت', 'بات', 'مناسب', 'نہیں۔', 'انھوں', 'دھرنے', 'قیادت', 'رابطہ', 'ہے،', 'بات', 'چیت', 'ہے،', 'دھرنے', 'قیادت', 'کمیٹی', 'مطالبات', 'ہیں،', 'عوام', 'ملک', 'بہتر', 'ہوگا', 'ریاست', 'کردار', 'ادا', 'گی۔', 'کشیدہ', 'صورت', 'حال،', 'ملک', 'مختلف', 'شہروں', 'اسکول', 'بند', 'اعلان', 'شہریار', 'آفریدی', 'کہنا', 'پاکستان', 'ادارے', 'معاملات', 'سمجھوتہ', 'گے،', 'وزیرِ', 'اعظم', 'واضح', 'پیغام', 'ریاست', 'ڈکٹیٹ', 'سکتا،', 'آسیہ', 'بی', 'بی', 'نام', 'ای', 'ایل', 'بات', 'آئی۔', 'وزیرِ', 'مملکت', 'اقلیتی', 'برادری', 'اتنی', 'عزت', 'جتنے', 'دیگر', 'شہریوں', 'ہے،', 'عمران', 'خان', 'وزیرِ', 'اعظم', 'بننے', 'یورپ', 'سوچ', 'بدلی', 'ہے،', 'حکومت', 'ہالینڈ', 'گستاخانہ', 'خاکوں', 'معاملے', 'خلاف', 'آواز', 'اٹھائی۔']\n",
            "['ملک', 'امن', 'امان', 'صورت', 'حال', 'کنٹرول', 'فواد', 'چوہدری', 'اسلام', 'وزیراطلاعات', 'فواد', 'چوہدری', 'کہنا', 'ملک', 'امن', 'امان', 'صورت', 'حال', 'کنٹرول', 'ہے،', 'ریاست', 'صبر', 'کمزوری', 'سمجھا', 'جائے۔', 'خیالات', 'اظہار', 'سماجی', 'رابطے', 'ویب', 'سائٹ', 'ٹوئٹر', 'جاری', 'کردہ', 'بیان', 'کیا،', 'فواد', 'چوہدری', 'کہنا', 'آئین', 'قانون', 'مکمل', 'عملداری', 'عوام', 'مفاد', 'ہے۔', 'فرض', 'پورا', 'گے،', 'وزیر', 'اعظم', 'صورت', 'حال', 'لمحہ', 'بہ', 'لمحہ', 'باخبر', 'جارہا', 'ہے۔', 'مزید', 'کہنا', 'چین', 'اہم', 'دورے', 'سبوتاژ', 'کوشش', 'ناکام', 'ہوگی۔', 'خوش', 'گمانی', 'ریاست', 'کمزورہے،', 'اپوزیشن', 'ادارے', 'حکومت', 'ساتھ', 'ہیں،', 'فواد', 'چوہدری', 'گذشتہ', 'روز', 'وزیراطلاعات', 'فواد', 'چوہدری', 'اپوزیشن', 'مذاکرات', 'کررہے', 'ہیں،', 'حکومت', 'حکمت', 'عملی', 'اپوزیشن', 'آگاہ', 'ہے،', 'اہم', 'ایشو', 'پورا', 'پاکستان', 'متحد', 'ہے۔', 'حکومت', 'ساتھ', 'چلنا', 'چاہتی', 'ہے،', 'حکومت', 'جانب', 'آئندہ', 'لائحہ', 'عممل', 'تیار', 'جارہا', 'ہے۔', 'کم', 'جونگ', 'جلد', 'جنوبی', 'کوریا', 'دورہ', 'پیانگ', 'شمالی', 'کوریا', 'رہنما', 'کم', 'جونگ', 'بار', 'جنوبی', 'کوریا', 'دورہ', 'دوران', 'اہم', 'ملاقاتیں', 'گی۔', 'تفصیلات', 'مطابق', 'جنوبی', 'کوریا', 'حکام', 'کہنا', 'کم', 'جونگ', 'جلد', 'ملک', 'دورہ', 'گے،', 'صدر', 'مون', 'جے', 'علاوہ', 'دیگر', 'اعلیٰ', 'عہدیداروں', 'ملاقاتیں', 'گی۔', 'غیر', 'ملکی', 'خبر', 'رساں', 'ادارے', 'مطابق', 'جنوبی', 'کوریا', 'صدر', 'مون', 'جے', 'کہنا', 'شمالی', 'کوریائی', 'رہنما', 'کم', 'جونگ', 'جلد', 'سیول', 'دورہ', 'گے۔', 'دورے', 'خطے', 'جوہری', 'ہتھیاروں', 'پاک', 'بنانے', 'حوالے', 'گفتگو', 'گی،', 'باہمی', 'تعلقات', 'مزید', 'مضبوط', 'بنانے', 'زور', 'گا۔', 'یاد', 'مون', 'جے', 'کم', 'جونگ', 'تاریخی', 'ملاقات', 'رواں', 'سال', 'اپریل', 'تھی،', 'صدر', 'شمالی', 'کوریا', 'خصوصی', 'ملاقات', 'جنوبی', 'کوریا', 'پہنچے', 'تھے۔', 'ازاں', 'رہنماؤں', 'مشترکہ', 'پریس', 'کانفرنس', 'باہمی', 'جنگ', 'خاتمے', 'اعلان', 'ساتھ', 'ساتھ', 'جوہری', 'ہتھیاروں', 'عدم', 'پھیلاؤ', 'اتفاق', 'تھا۔', 'کم', 'جونگ', 'جنوبی', 'کوریا', 'آمد،', 'پرتباک', 'استقبال،', 'تاریخی', 'ملاقات', 'خیال', 'ممالک', 'گذشتہ', 'کئی', 'سالوں', 'دوسرے', 'حریف', 'ہیں،', 'ماضی', 'دوسرے', 'ملکوں', 'تباہ', 'سنگین', 'دھمکیاں', 'ہیں۔', 'واضح', 'شمالی', 'کوریا', 'جانب', 'اعلان', 'جوہری', 'پروگرام', 'ترک', 'کردیا', 'ہے،', 'ازاں', 'حکام', 'بین', 'الاقوامی', 'معائنہ', 'کاروں', 'جوہری', 'تجربہ', 'گاہ', 'معائنے', 'اجازت', 'ہے۔', 'چینی', 'صدر', 'امریکی', 'منصب', 'ٹیلی', 'فونک', 'رابطہ،', 'تجارتی', 'معاملات', 'گفتگو', 'چینی', 'صدر', 'شی', 'پنگ', 'امریکی', 'منصب', 'ڈونلڈ', 'ٹرمپ', 'ٹیلی', 'فونک', 'رابطہ', 'ہوا،', 'دوران', 'تجارتی', 'معاملات', 'تبادلہ', 'خیال', 'گیا۔', 'تفصیلات', 'مطابق', 'ٹیلی', 'فونک', 'گفتگو', 'دوران', 'ملکوں', 'صدور', 'تجارتی', 'معاملات', 'علاوہ', 'شمالی', 'کوریا', 'موضوع', 'گفتگو', 'کی۔', 'غیر', 'ملکی', 'خبر', 'رساں', 'ادارے', 'مطابق', 'چینی', 'صدر', 'شی', 'پنگ', 'کہنا', 'چین', 'خطے', 'ہمیشہ', 'امن', 'استحکام', 'بات', 'ہے،', 'عالمی', 'سطح', 'قیام', 'امن', 'کوششیں', 'جاری', 'رکھیں', 'گے۔', 'امریکی', 'صدر', 'گفتگو', 'دوران', 'چینی', 'صدر', 'مزید', 'کہنا', 'جزیرہ', 'نما', 'کوریا', 'جوہری', 'ہتھیاروں', 'صاف', 'بنانے', 'بھرپور', 'مدد', 'فراہم', 'گی۔', 'دوسری', 'جانب', 'امریکی', 'صدر', 'ڈونلڈ', 'ٹرمپ', 'جاری', 'کردہ', 'بیان', 'واضح', 'چینی', 'صدر', 'شی', 'پنگ', 'ساتھ', 'بات', 'چیت', 'اچھی', 'رہی۔', 'چین', 'امریکا', 'تعلقات', 'کشیدگی', 'ختم', 'اہم', 'اقدامات', 'ملکوں', 'سربراہان', 'جلد', 'ملاقات', 'اتفاق', 'ہے۔', 'واضح', 'حالیہ', 'چند', 'ہفتوں', 'چین', 'امریکا', 'درمیان', 'شدید', 'تجارتی', 'جنگ', 'جاری', 'ہے۔', 'خیال', 'رواں', 'سال', 'جون', 'امریکا', 'پہلی', 'مرتبہ', 'چینی', 'مصنوعات', 'چونتیس', 'بلین', 'ڈالر', 'اضافی', 'محصولات', 'عائد', 'فوری', 'ردعمل', 'چین', 'امریکی', 'مصنوعات', 'ساٹھ', 'بلین', 'ڈالر', 'اضافی', 'ٹیکس', 'عائد', 'اعلان', 'تھا۔', 'یاد', 'رواں', 'سال', 'اگست', 'امریکی', 'حکام', 'جانب', 'چینی', 'درآمدات', 'مزید', 'ارب', 'ڈالر', 'اضافی', 'ٹیکس', 'عائد', 'تھے۔', 'فاٹا', 'خیبر', 'پختونخوا', 'انضمام', 'اکثریت', 'فیصلہ', 'ہے،', 'وزیرِ', 'اطلاعات', 'شوکت', 'یوسف', 'زئی', 'صوبائی', 'وزیرِ', 'اطلاعات', 'شوکت', 'یوسف', 'زئی', 'فاٹا', 'خیبر', 'پختونخوا', 'انضمام', 'اکثریت', 'فیصلہ', 'ہے۔', 'صوبائی', 'وزیرِ', 'اطلاعات', 'مٹھی', 'بھر', 'عناصر', 'بیرونی', 'مفادات', 'فاٹا', 'انضمام', 'مخالفت', 'ہیں،', 'پی', 'فاٹا', 'انضمام', 'اکثریت', 'فیصلہ', 'ہے۔', 'قبائلی', 'روایات', 'مطابق', 'ڈی', 'آر', 'سیز', 'دائرہ', 'کار', 'فاٹا', 'پھیلائیں', 'وزیرِ', 'اطلاعات', 'شوکت', 'یوسف', 'زئی', 'کہنا', 'حکومت', 'سنجیدہ', 'ہے،', 'فاٹا', 'اصلاحات', 'تیزی', 'حکومت', 'سنجیدہ', 'اقدامات', 'ہے۔', 'وزیرِ', 'اطلاعات', 'قبائلی', 'روایات', 'مطابق', 'ڈی', 'آر', 'سیز', 'دائرہ', 'کار', 'فاٹا', 'پھیلائیں', 'گے،', 'قبائلی', 'علاقوں', 'صحت', 'تعلیم', 'معیار', 'بہتر', 'بنا', 'ہیں۔', 'صوبائی', 'وزیر', 'شوکت', 'یوسف', 'زئی', 'کہنا', 'قبائلی', 'عوام', 'فوری', 'انصاف', 'فراہمی', 'حکومتی', 'ترجیحات', 'سرِ', 'فہرست', 'ہے۔', 'یاد', 'ماہ', 'قبل', 'گورنر', 'خیبر', 'پختونخوا', 'سمیت', 'فاٹا', 'ارکانِ', 'قومی', 'اسمبلی', 'وزیرِ', 'اعظم', 'عمران', 'خان', 'مطالبہ', 'فاٹا', 'قومی', 'مالیاتی', 'کمیشن', 'ایف', 'ایوارڈ', 'تین', 'فی', 'صد', 'فوری', 'طور', 'ادا', 'جائے۔', 'این', 'ایف', 'ایوارڈ', 'فی', 'صد', 'حصہ', 'جلد', 'از', 'جلد', 'فاٹا', 'جائے،', 'فاٹا', 'ارکان', 'وزیرِ', 'اعظم', 'عمران', 'خان', 'مطالبہ', 'ارکان', 'عمران', 'خان', 'مطالبہ', 'فاٹا', 'علاقوں', 'ایم', 'پی', 'ایز', 'تعداد', 'بڑھا', 'جائے،', 'فاٹا', 'ایم', 'این', 'ایز', 'موجودہ', 'تعداد', 'برقرار', 'جائے۔', 'فاٹا', 'ارکان', 'وزیرِ', 'اعظم', 'عمران', 'خان', 'مطالبہ', 'ڈی', 'پی', 'ترقیاتی', 'مختص', 'رقم', 'جلد', 'از', 'جلد', 'جاری', 'جائے۔', 'پوری', 'کوشش', 'معاملے', 'افہام', 'تفہیم', 'حل', 'کریں،', 'وزیرِ', 'مملکت', 'برائے', 'داخلہ', 'اسلام', 'وزیرِ', 'مملکت', 'برائے', 'داخلہ', 'شہریار', 'آفریدی', 'سپریم', 'کورٹ', 'فیصلے', 'ملک', 'پیدا', 'صورتِ', 'حال', 'پوری', 'کوشش', 'معاملے', 'افہام', 'تفہیم', 'حل', 'کریں۔', 'شہریار', 'آفریدی', 'آر', 'وائی', 'نیوز', 'گفتگو', 'آج', 'رات', 'معاملات', 'حل', 'ہوجائیں', 'گے،', 'قانون', 'ہاتھ', 'لینے', 'لوگ', 'گرفتار', 'ہیں،', 'پوری', 'کوشش', 'معاملے', 'افہام', 'تفہیم', 'حل', 'کریں۔', 'موبائل', 'سروس', 'بند', 'ش', 'متعلق', 'معاملے', 'علم', 'شہریار', 'آفریدی', 'وزیر', 'مملکت', 'موبائل', 'سروس', 'بند', 'ش', 'متعلق', 'معاملے', 'علم', 'نہیں،', 'دستخط', 'بغیر', 'اقدام', 'گا،', 'ریاستی', 'امور', 'وقت', 'بات', 'مناسب', 'نہیں۔', 'انھوں', 'دھرنے', 'قیادت', 'رابطہ', 'ہے،', 'بات', 'چیت', 'ہے،', 'دھرنے', 'قیادت', 'کمیٹی', 'مطالبات', 'ہیں،', 'عوام', 'ملک', 'بہتر', 'ہوگا', 'ریاست', 'کردار', 'ادا', 'گی۔', 'کشیدہ', 'صورت', 'حال،', 'ملک', 'مختلف', 'شہروں', 'اسکول', 'بند', 'اعلان', 'شہریار', 'آفریدی', 'کہنا', 'پاکستان', 'ادارے', 'معاملات', 'سمجھوتہ', 'گے،', 'وزیرِ', 'اعظم', 'واضح', 'پیغام', 'ریاست', 'ڈکٹیٹ', 'سکتا،', 'آسیہ', 'بی', 'بی', 'نام', 'ای', 'ایل', 'بات', 'آئی۔', 'وزیرِ', 'مملکت', 'اقلیتی', 'برادری', 'اتنی', 'عزت', 'جتنے', 'دیگر', 'شہریوں', 'ہے،', 'عمران', 'خان', 'وزیرِ', 'اعظم', 'بننے', 'یورپ', 'سوچ', 'بدلی', 'ہے،', 'حکومت', 'ہالینڈ', 'گستاخانہ', 'خاکوں', 'معاملے', 'خلاف', 'آواز', 'اٹھائی۔']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import ISRIStemmer"
      ],
      "metadata": {
        "id": "iKWP39f_Hpz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=ISRIStemmer()"
      ],
      "metadata": {
        "id": "fh0EbXUFI_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REMOVING URDU STOPWORDS**"
      ],
      "metadata": {
        "id": "kzftKK9zYkGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_urdu_word(word):\n",
        "\n",
        "    #define a regex pattern to match Urdu characters\n",
        "    pattern = re.compile(\"^[\\u0600-\\u06FF]+$\")\n",
        "    return pattern.match(word) is not None"
      ],
      "metadata": {
        "id": "QG7dUofSYzFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_urdu_words(filename):\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    words = contents.split()\n",
        "    urdu_words = [word for word in words if is_urdu_word(word)]\n",
        "\n",
        "    #Reading the stopwords file\n",
        "    with open(\"Urdu stopwords.txt\", \"r\") as f:\n",
        "        stopwords = f.read().split()\n",
        "\n",
        "    filtered_urdu_words = [word for word in urdu_words if word not in stopwords]\n",
        "\n",
        "    #Joining the filtered words back into a string\n",
        "    filtered_contents = \" \".join(filtered_urdu_words)\n",
        "    print(filtered_contents)\n",
        "\n",
        "    # write the filtered string back to the file\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(filtered_contents)"
      ],
      "metadata": {
        "id": "1KIm1XlrY0DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\"1.txt\", \"2.txt\", \"3.txt\", \"4.txt\", \"5.txt\"]\n",
        "\n",
        "for filename in filenames:\n",
        "    if os.path.exists(filename):\n",
        "        remove_non_urdu_words(filename)\n",
        "    else:\n",
        "        print(f\"{filename} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX8cvT0sZPGN",
        "outputId": "cfec68a2-dc1d-43d8-cd9d-a0de54162723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ملک امن امان صورت حال کنٹرول فواد چوہدری اسلام وزیراطلاعات فواد چوہدری کہنا ملک امن امان صورت حال کنٹرول ہے، ریاست صبر کمزوری سمجھا جائے۔ خیالات اظہار سماجی رابطے ویب سائٹ ٹوئٹر جاری کردہ بیان کیا، فواد چوہدری کہنا آئین قانون مکمل عملداری عوام مفاد ہے۔ فرض پورا گے، وزیر اعظم صورت حال لمحہ بہ لمحہ باخبر جارہا ہے۔ مزید کہنا چین اہم دورے سبوتاژ کوشش ناکام ہوگی۔ خوش گمانی ریاست کمزورہے، اپوزیشن ادارے حکومت ساتھ ہیں، فواد چوہدری گذشتہ روز وزیراطلاعات فواد چوہدری اپوزیشن مذاکرات کررہے ہیں، حکومت حکمت عملی اپوزیشن آگاہ ہے، اہم ایشو پورا پاکستان متحد ہے۔ حکومت ساتھ چلنا چاہتی ہے، حکومت جانب آئندہ لائحہ عممل تیار جارہا ہے۔\n",
            "کم جونگ جلد جنوبی کوریا دورہ پیانگ شمالی کوریا رہنما کم جونگ بار جنوبی کوریا دورہ دوران اہم ملاقاتیں گی۔ تفصیلات مطابق جنوبی کوریا حکام کہنا کم جونگ جلد ملک دورہ گے، صدر مون جے علاوہ دیگر اعلیٰ عہدیداروں ملاقاتیں گی۔ غیر ملکی خبر رساں ادارے مطابق جنوبی کوریا صدر مون جے کہنا شمالی کوریائی رہنما کم جونگ جلد سیول دورہ گے۔ دورے خطے جوہری ہتھیاروں پاک بنانے حوالے گفتگو گی، باہمی تعلقات مزید مضبوط بنانے زور گا۔ یاد مون جے کم جونگ تاریخی ملاقات رواں سال اپریل تھی، صدر شمالی کوریا خصوصی ملاقات جنوبی کوریا پہنچے تھے۔ ازاں رہنماؤں مشترکہ پریس کانفرنس باہمی جنگ خاتمے اعلان ساتھ ساتھ جوہری ہتھیاروں عدم پھیلاؤ اتفاق تھا۔ کم جونگ جنوبی کوریا آمد، پرتباک استقبال، تاریخی ملاقات خیال ممالک گذشتہ کئی سالوں دوسرے حریف ہیں، ماضی دوسرے ملکوں تباہ سنگین دھمکیاں ہیں۔ واضح شمالی کوریا جانب اعلان جوہری پروگرام ترک کردیا ہے، ازاں حکام بین الاقوامی معائنہ کاروں جوہری تجربہ گاہ معائنے اجازت ہے۔\n",
            "چینی صدر امریکی منصب ٹیلی فونک رابطہ، تجارتی معاملات گفتگو چینی صدر شی پنگ امریکی منصب ڈونلڈ ٹرمپ ٹیلی فونک رابطہ ہوا، دوران تجارتی معاملات تبادلہ خیال گیا۔ تفصیلات مطابق ٹیلی فونک گفتگو دوران ملکوں صدور تجارتی معاملات علاوہ شمالی کوریا موضوع گفتگو کی۔ غیر ملکی خبر رساں ادارے مطابق چینی صدر شی پنگ کہنا چین خطے ہمیشہ امن استحکام بات ہے، عالمی سطح قیام امن کوششیں جاری رکھیں گے۔ امریکی صدر گفتگو دوران چینی صدر مزید کہنا جزیرہ نما کوریا جوہری ہتھیاروں صاف بنانے بھرپور مدد فراہم گی۔ دوسری جانب امریکی صدر ڈونلڈ ٹرمپ جاری کردہ بیان واضح چینی صدر شی پنگ ساتھ بات چیت اچھی رہی۔ چین امریکا تعلقات کشیدگی ختم اہم اقدامات ملکوں سربراہان جلد ملاقات اتفاق ہے۔ واضح حالیہ چند ہفتوں چین امریکا درمیان شدید تجارتی جنگ جاری ہے۔ خیال رواں سال جون امریکا پہلی مرتبہ چینی مصنوعات چونتیس بلین ڈالر اضافی محصولات عائد فوری ردعمل چین امریکی مصنوعات ساٹھ بلین ڈالر اضافی ٹیکس عائد اعلان تھا۔ یاد رواں سال اگست امریکی حکام جانب چینی درآمدات مزید ارب ڈالر اضافی ٹیکس عائد تھے۔\n",
            "فاٹا خیبر پختونخوا انضمام اکثریت فیصلہ ہے، وزیرِ اطلاعات شوکت یوسف زئی صوبائی وزیرِ اطلاعات شوکت یوسف زئی فاٹا خیبر پختونخوا انضمام اکثریت فیصلہ ہے۔ صوبائی وزیرِ اطلاعات مٹھی بھر عناصر بیرونی مفادات فاٹا انضمام مخالفت ہیں، پی فاٹا انضمام اکثریت فیصلہ ہے۔ قبائلی روایات مطابق ڈی آر سیز دائرہ کار فاٹا پھیلائیں وزیرِ اطلاعات شوکت یوسف زئی کہنا حکومت سنجیدہ ہے، فاٹا اصلاحات تیزی حکومت سنجیدہ اقدامات ہے۔ وزیرِ اطلاعات قبائلی روایات مطابق ڈی آر سیز دائرہ کار فاٹا پھیلائیں گے، قبائلی علاقوں صحت تعلیم معیار بہتر بنا ہیں۔ صوبائی وزیر شوکت یوسف زئی کہنا قبائلی عوام فوری انصاف فراہمی حکومتی ترجیحات سرِ فہرست ہے۔ یاد ماہ قبل گورنر خیبر پختونخوا سمیت فاٹا ارکانِ قومی اسمبلی وزیرِ اعظم عمران خان مطالبہ فاٹا قومی مالیاتی کمیشن ایف ایوارڈ تین فی صد فوری طور ادا جائے۔ این ایف ایوارڈ فی صد حصہ جلد از جلد فاٹا جائے، فاٹا ارکان وزیرِ اعظم عمران خان مطالبہ ارکان عمران خان مطالبہ فاٹا علاقوں ایم پی ایز تعداد بڑھا جائے، فاٹا ایم این ایز موجودہ تعداد برقرار جائے۔ فاٹا ارکان وزیرِ اعظم عمران خان مطالبہ ڈی پی ترقیاتی مختص رقم جلد از جلد جاری جائے۔\n",
            "پوری کوشش معاملے افہام تفہیم حل کریں، وزیرِ مملکت برائے داخلہ اسلام وزیرِ مملکت برائے داخلہ شہریار آفریدی سپریم کورٹ فیصلے ملک پیدا صورتِ حال پوری کوشش معاملے افہام تفہیم حل کریں۔ شہریار آفریدی آر وائی نیوز گفتگو آج رات معاملات حل ہوجائیں گے، قانون ہاتھ لینے لوگ گرفتار ہیں، پوری کوشش معاملے افہام تفہیم حل کریں۔ موبائل سروس بند ش متعلق معاملے علم شہریار آفریدی وزیر مملکت موبائل سروس بند ش متعلق معاملے علم نہیں، دستخط بغیر اقدام گا، ریاستی امور وقت بات مناسب نہیں۔ انھوں دھرنے قیادت رابطہ ہے، بات چیت ہے، دھرنے قیادت کمیٹی مطالبات ہیں، عوام ملک بہتر ہوگا ریاست کردار ادا گی۔ کشیدہ صورت حال، ملک مختلف شہروں اسکول بند اعلان شہریار آفریدی کہنا پاکستان ادارے معاملات سمجھوتہ گے، وزیرِ اعظم واضح پیغام ریاست ڈکٹیٹ سکتا، آسیہ بی بی نام ای ایل بات آئی۔ وزیرِ مملکت اقلیتی برادری اتنی عزت جتنے دیگر شہریوں ہے، عمران خان وزیرِ اعظم بننے یورپ سوچ بدلی ہے، حکومت ہالینڈ گستاخانہ خاکوں معاملے خلاف آواز اٹھائی۔\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GENERATING DOCIDS.TXT**"
      ],
      "metadata": {
        "id": "VlWRVFkIZV2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_docid_file(filenames):\n",
        "\n",
        "    #Create a mapping from filenames to DOCIDs\n",
        "    docid_map = {filename: i + 1 for i, filename in enumerate(filenames)}\n",
        "\n",
        "    #Writing the mapping to a file\n",
        "    with open(\"docids.txt\", \"w\") as f:\n",
        "        for filename, docid in docid_map.items():\n",
        "            f.write(f\"{docid}\\t{filename}\\n\")\n",
        "\n",
        "filenames = [\"1.txt\", \"2.txt\", \"3.txt\", \"4.txt\", \"5.txt\"]\n",
        "generate_docid_file(filenames)"
      ],
      "metadata": {
        "id": "F_77wqDR6CB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GENERATING TERMIDS.TXT**"
      ],
      "metadata": {
        "id": "aiF9uVvmaWap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def tokenize_text(filename):\n",
        "\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"{filename} does not exist.\")\n",
        "        return\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    words = text.split()\n",
        "\n",
        "    #Storing the unique words and their frequency\n",
        "    term_frequency = defaultdict(int)\n",
        "    for word in words:\n",
        "        term_frequency[word] += 1\n",
        "\n",
        "    #Writing the unique words and their term IDs to the termids.txt file\n",
        "    with open(\"termids.txt\", \"w\") as f:\n",
        "        for i, (word, _) in enumerate(term_frequency.items()):\n",
        "            f.write(f\"{i+1}\\t{word}\\n\")\n",
        "\n",
        "filenames = [\"1.txt\", \"2.txt\", \"3.txt\", \"4.txt\", \"5.txt\"]\n",
        "for filename in filenames:\n",
        "    tokenize_text(filename)\n",
        ""
      ],
      "metadata": {
        "id": "agRUyHPW61F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GENERATING DOC_INDEX.TXT**"
      ],
      "metadata": {
        "id": "bD_iMkejOBP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(filename):\n",
        "\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"{filename} does not exist.\")\n",
        "        return\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    words = text.split()\n",
        "\n",
        "    term_frequency = defaultdict(int)\n",
        "    for word in words:\n",
        "        term_frequency[word] += 1\n",
        "\n",
        "    term_ids = {word: i+1 for i, (word, _) in enumerate(term_frequency.items())}\n",
        "\n",
        "    # Write the doc ID, term ID, and positions of each term in the document to the doc_index.txt file\n",
        "    with open(\"doc_index.txt\", \"a\") as f:\n",
        "      #Changing the mode to \"a\" to append instead of overwrite\n",
        "\n",
        "        doc_id = filenames.index(filename) + 1\n",
        "         #Using the index of the filename in the filenames list as the document ID\n",
        "        for i, word in enumerate(words):\n",
        "            term_id = term_ids[word]\n",
        "            f.write(f\"{doc_id} {term_id} {i+1}\\n\")\n",
        "\n",
        "filenames = [\"1.txt\", \"2.txt\", \"3.txt\", \"4.txt\", \"5.txt\"]\n",
        "\n",
        "# Call the tokenize_text function for each file\n",
        "for filename in filenames:\n",
        "    tokenize_text(filename)"
      ],
      "metadata": {
        "id": "LvnrKdnmCDeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 2**\n"
      ],
      "metadata": {
        "id": "qw5jmGhDd-rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DELTA ENCODING**"
      ],
      "metadata": {
        "id": "_DLPWsmUeEKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"doc_index.txt\") as doc_file:\n",
        "    # create a list to store the contents\n",
        "    content = []\n",
        "\n",
        "    #loop over each line in the file\n",
        "    for line in doc_file:\n",
        "        #remove any trailing whitespace from the line and split it into a list of strings\n",
        "        parts = line.strip().split()\n",
        "\n",
        "        # append the list of strings to the content list\n",
        "        content.append(parts)"
      ],
      "metadata": {
        "id": "kE06DQLjCo6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_index = {}\n",
        "\n",
        "for doc in content:\n",
        "    doc_id, term_id, *positions = doc\n",
        "\n",
        "    # Delta-encode positions list\n",
        "    delta_positions = [positions[0]] + [int(positions[i]) - int(positions[i-1]) for i in range(1, len(positions))]\n",
        "\n",
        "    if term_id not in term_index:\n",
        "        term_index[term_id] = {}\n",
        "\n",
        "    term_index[term_id][doc_id] = delta_positions"
      ],
      "metadata": {
        "id": "GEVsMJjsCq6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING TERM_INDEX.TXT**"
      ],
      "metadata": {
        "id": "9he1mgVfOOcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"term_index.txt\", \"w\") as term_file:\n",
        "    for t_id, d_id_to_pos in term_index.items():\n",
        "        l = [str(t_id)]\n",
        "        prev_d_id = 0\n",
        "        for d_id, pos in d_id_to_pos.items():\n",
        "            delta_d_id = int(d_id) - prev_d_id\n",
        "            delta_pos = [str(pos[0])] + [str(pos[i] - pos[i-1]) for i in range(1, len(pos))]\n",
        "            positions_str = \",\".join(delta_pos)\n",
        "            l.append(f\"{delta_d_id}:{positions_str}\")\n",
        "            prev_d_id = int(d_id)\n",
        "        term_file.write(\"\\t\".join(l) + \"\\n\")"
      ],
      "metadata": {
        "id": "Ky88peaXCyls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING TERM_INFO.TXT**"
      ],
      "metadata": {
        "id": "SbLXOpfYOUAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"term_info.txt\", \"w\") as info:\n",
        "    offset = 0\n",
        "    for t_id, d_id_to_pos in term_index.items():\n",
        "        occ = sum(len(pos) for pos in d_id_to_pos.values())\n",
        "        docs = len(d_id_to_pos)\n",
        "        l = f\"{t_id}\\t{offset}\\t{occ}\\t{docs}\\n\"\n",
        "        info.write(l)\n",
        "        offset += occ * 2 + docs * 4 + 1"
      ],
      "metadata": {
        "id": "TsDl2CfGC2UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 3**"
      ],
      "metadata": {
        "id": "G1e20ofqD6JO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--term\")\n",
        "parser.add_argument(\"--doc\")\n",
        "parser.add_argument(\"--docs\", action=\"store_true\")\n",
        "parser.add_argument(\"--terms\", action=\"store_true\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "doc_index_file = \"doc_index.txt\"\n",
        "doc_ids_file = \"docids.txt\"\n",
        "\n",
        "term_index_file = \"term_index.txt\"\n",
        "term_info_file = \"term_info.txt\"\n",
        "term_ids_file = \"termids.txt\"\n",
        "\n",
        "doc_ids = dict()\n",
        "term_ids = dict()\n",
        "doc_index = dict()\n",
        "term_info = dict()\n",
        "\n",
        "with open(doc_ids_file, mode='r', encoding='utf-8') as f:\n",
        "    for row in f:\n",
        "        row = row.strip().split()\n",
        "        doc_id = int(row[0])\n",
        "        doc_name = row[1]\n",
        "        doc_ids[doc_name] = doc_id\n",
        "\n",
        "with open(term_ids_file, mode='r', encoding='utf-8') as f:\n",
        "    for row in f:\n",
        "        row = row.strip().split()\n",
        "        term_id = int(row[0])\n",
        "        term = row[1]\n",
        "        term_ids[term] = term_id\n",
        "\n",
        "with open(doc_index_file, mode='r', encoding='utf-8') as f:\n",
        "    for row in f:\n",
        "        row = row.strip().split()\n",
        "        doc_id = int(row[0])\n",
        "        term_id = int(row[1])\n",
        "        term_positions = row[2:]\n",
        "\n",
        "        for i in range(len(term_positions)):\n",
        "            term_positions[i] = int(term_positions[i])\n",
        "\n",
        "        if doc_id not in doc_index:\n",
        "            doc_index[doc_id] = dict()\n",
        "\n",
        "        doc_index[doc_id][term_id] = term_positions\n",
        "\n",
        "with open(term_info_file, mode='r', encoding='utf-8') as f:\n",
        "    for row in f:\n",
        "        row = row.strip().split()\n",
        "        term_id = int(row[0])\n",
        "        index_offset = int(row[1])\n",
        "        term_corpus_freq = int(row[2])\n",
        "        term_total_docs = int(row[3])\n",
        "\n",
        "        term_info[term_id] = {\"index_offset\" : index_offset,\n",
        "                              \"term_corpus_freq\" : term_corpus_freq,\n",
        "                              \"document_freq\" : term_total_docs}\n",
        "\n",
        "if args.docs:\n",
        "    for doc_name in doc_ids.keys():\n",
        "        doc_id = doc_ids[doc_name]\n",
        "        distinct_terms = len(doc_index[doc_id])\n",
        "        total_terms = 0\n",
        "        for key in doc_index[doc_id].keys():\n",
        "            total_terms += len(doc_index[doc_id][key])\n",
        "\n",
        "        print(\"doc name: \", doc_name)\n",
        "        print(\"doc id: \", doc_id)\n",
        "        print(\"distinct terms: \", distinct_terms)\n",
        "        print(\"total terms: \", total_terms)\n",
        "\n",
        "elif args.terms:\n",
        "    for term in term_ids.keys():\n",
        "        term_id = term_ids[term]\n",
        "        index_offset = term_info[term_id][\"index_offset\"]\n",
        "        term_corpus_freq = term_info[term_id][\"term_corpus_freq\"]\n",
        "        term_total_docs = term_info[term_id][\"document_freq\"]\n",
        "\n",
        "        print(\"term: \", term)\n",
        "        print(\"term id: \", term_id)\n",
        "        print(\"index offset: \", index_offset)\n",
        "        print(\"term corpus freq: \", term_corpus_freq)\n",
        "        print(\"term total docs: \", term_total_docs)\n",
        "\n",
        "elif args.doc != None and args.term != None:\n",
        "    doc_name = args.doc\n",
        "    doc_id = doc_ids[doc_name]\n",
        "    term = args.term\n",
        "    term_id = term_ids[term]\n",
        "    positions = list()\n",
        "    offset = term_info[term_id][\"index_offset\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "G6u8yiMxhvBD",
        "outputId": "dcc071a5-12e4-4c68-e43d-5ef8fa074b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--term TERM] [--doc DOC] [--docs] [--terms]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-15e5d86c-a855-4664-bd75-adcd1a2fe893.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApZOBcN6ZJkt",
        "outputId": "3d53ad77-98df-494a-e57d-83f41d111e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.9/dist-packages (0.8.11)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from python-docx) (4.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the Excel file\n",
        "qrels_file = pd.ExcelFile('qrels.xlsx')\n",
        "\n",
        "# Loop through all the sheets in the file\n",
        "for sheet_name in qrels_file.sheet_names:\n",
        "    print(f'Printing sheet: {sheet_name}')\n",
        "\n",
        "    # Read the sheet into a DataFrame\n",
        "    sheet_df = pd.read_excel(qrels_file, sheet_name=sheet_name)\n",
        "\n",
        "    # Print the entire sheet\n",
        "    print(sheet_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cryM1qiUeL13",
        "outputId": "68b895d6-d2e4-4246-8996-5d39522802e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing sheet: Q1 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         1  2368653              4            50.0   \n",
            "1          2         1  2362784              4             NaN   \n",
            "2          3         1   619750              4             NaN   \n",
            "3          4         1  2378593              4             NaN   \n",
            "4          5         1  2008560              4             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "475      476         1  2029196              4             NaN   \n",
            "476      477         1  1944851              3             NaN   \n",
            "477      478         1  2004171              4             NaN   \n",
            "478      479         1  1679235              3             NaN   \n",
            "479      480         1  2385089              4             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                      113.0                131.0                186.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "475                      NaN                  NaN                  NaN  \n",
            "476                      NaN                  NaN                  NaN  \n",
            "477                      NaN                  NaN                  NaN  \n",
            "478                      NaN                  NaN                  NaN  \n",
            "479                      NaN                  NaN                  NaN  \n",
            "\n",
            "[480 rows x 8 columns]\n",
            "Printing sheet: Q2 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         2  1799751              4            29.0   \n",
            "1          2         2  2462801              4             NaN   \n",
            "2          3         2  1171970              3             NaN   \n",
            "3          4         2  2463347              4             NaN   \n",
            "4          5         2  2494632              3             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "455      456         2  2875050              4             NaN   \n",
            "456      457         2  1907570              4             NaN   \n",
            "457      458         2  2574885              4             NaN   \n",
            "458      459         2  2136515              4             NaN   \n",
            "459      460         2  2133624              4             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                      117.0                101.0                213.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "455                      NaN                  NaN                  NaN  \n",
            "456                      NaN                  NaN                  NaN  \n",
            "457                      NaN                  NaN                  NaN  \n",
            "458                      NaN                  NaN                  NaN  \n",
            "459                      NaN                  NaN                  NaN  \n",
            "\n",
            "[460 rows x 8 columns]\n",
            "Printing sheet: Q3 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         3  2666140              3            38.0   \n",
            "1          2         3  2171441              3             NaN   \n",
            "2          3         3  1546532              4             NaN   \n",
            "3          4         3  2173545              2             NaN   \n",
            "4          5         3  2075078              3             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "267      268         3   278399              3             NaN   \n",
            "268      269         3  2656883              3             NaN   \n",
            "269      270         3  1311204              2             NaN   \n",
            "270      271         3  2159678              2             NaN   \n",
            "271      272         3  1367132              2             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                       98.0                 93.0                 43.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "267                      NaN                  NaN                  NaN  \n",
            "268                      NaN                  NaN                  NaN  \n",
            "269                      NaN                  NaN                  NaN  \n",
            "270                      NaN                  NaN                  NaN  \n",
            "271                      NaN                  NaN                  NaN  \n",
            "\n",
            "[272 rows x 8 columns]\n",
            "Printing sheet: Q4 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         4  2477554              3            34.0   \n",
            "1          2         4  2727892              3             NaN   \n",
            "2          3         4  2503741              4             NaN   \n",
            "3          4         4  2500795              4             NaN   \n",
            "4          5         4  2501404              4             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "402      403         4  2820159              1             NaN   \n",
            "403      404         4  2367571              4             NaN   \n",
            "404      405         4   582856              2             NaN   \n",
            "405      406         4   817176              4             NaN   \n",
            "406      407         4   235842              4             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                       58.0                 96.0                219.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "402                      NaN                  NaN                  NaN  \n",
            "403                      NaN                  NaN                  NaN  \n",
            "404                      NaN                  NaN                  NaN  \n",
            "405                      NaN                  NaN                  NaN  \n",
            "406                      NaN                  NaN                  NaN  \n",
            "\n",
            "[407 rows x 8 columns]\n",
            "Printing sheet: Q5 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         5  2656088              2            34.0   \n",
            "1          2         5   698931              4             NaN   \n",
            "2          3         5  1930101              2             NaN   \n",
            "3          4         5   699090              4             NaN   \n",
            "4          5         5   954117              3             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "423      424         5   874962              3             NaN   \n",
            "424      425         5  2436051              3             NaN   \n",
            "425      426         5  2096397              2             NaN   \n",
            "426      427         5   843172              4             NaN   \n",
            "427      428         5   605508              2             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                      163.0                113.0                118.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "423                      NaN                  NaN                  NaN  \n",
            "424                      NaN                  NaN                  NaN  \n",
            "425                      NaN                  NaN                  NaN  \n",
            "426                      NaN                  NaN                  NaN  \n",
            "427                      NaN                  NaN                  NaN  \n",
            "\n",
            "[428 rows x 8 columns]\n",
            "Printing sheet: Q6 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         6  2541010              4           142.0   \n",
            "1          2         6  1331199              2             NaN   \n",
            "2          3         6  1949773              4             NaN   \n",
            "3          4         6  2549885              3             NaN   \n",
            "4          5         6  2635921              4             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "535      536         6   182921              4             NaN   \n",
            "536      537         6  1448169              4             NaN   \n",
            "537      538         6  1450609              1             NaN   \n",
            "538      539         6   168684              3             NaN   \n",
            "539      540         6   168805              3             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                      169.0                140.0                 89.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "535                      NaN                  NaN                  NaN  \n",
            "536                      NaN                  NaN                  NaN  \n",
            "537                      NaN                  NaN                  NaN  \n",
            "538                      NaN                  NaN                  NaN  \n",
            "539                      NaN                  NaN                  NaN  \n",
            "\n",
            "[540 rows x 8 columns]\n",
            "Printing sheet: Q7 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         7  1043001              3           101.0   \n",
            "1          2         7  1041117              1             NaN   \n",
            "2          3         7  1042187              1             NaN   \n",
            "3          4         7  2402651              1             NaN   \n",
            "4          5         7   417147              1             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "449      450         7   154798              2             NaN   \n",
            "450      451         7   842355              2             NaN   \n",
            "451      452         7  1436518              4             NaN   \n",
            "452      453         7  2123460              3             NaN   \n",
            "453      454         7  1028372              2             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                      153.0                111.0                 89.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "449                      NaN                  NaN                  NaN  \n",
            "450                      NaN                  NaN                  NaN  \n",
            "451                      NaN                  NaN                  NaN  \n",
            "452                      NaN                  NaN                  NaN  \n",
            "453                      NaN                  NaN                  NaN  \n",
            "\n",
            "[454 rows x 8 columns]\n",
            "Printing sheet: Q8 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         8  2087992              2            71.0   \n",
            "1          2         8  1254354              2             NaN   \n",
            "2          3         8   922866              3             NaN   \n",
            "3          4         8   672810              4             NaN   \n",
            "4          5         8   672450              4             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "435      436         8   126081              2             NaN   \n",
            "436      437         8   126523              4             NaN   \n",
            "437      438         8   827053              2             NaN   \n",
            "438      439         8   905152              2             NaN   \n",
            "439      440         8   917325              2             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                      155.0                 89.0                125.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "435                      NaN                  NaN                  NaN  \n",
            "436                      NaN                  NaN                  NaN  \n",
            "437                      NaN                  NaN                  NaN  \n",
            "438                      NaN                  NaN                  NaN  \n",
            "439                      NaN                  NaN                  NaN  \n",
            "\n",
            "[440 rows x 8 columns]\n",
            "Printing sheet: Q9 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1         9   246723              1            90.0   \n",
            "1          2         9  1275165              3             NaN   \n",
            "2          3         9  1165055              1             NaN   \n",
            "3          4         9  1915300              2             NaN   \n",
            "4          5         9  1618023              2             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "411      412         9    19704              3             NaN   \n",
            "412      413         9  1660520              3             NaN   \n",
            "413      414         9   976021              2             NaN   \n",
            "414      415         9   143629              3             NaN   \n",
            "415      416         9  2020963              2             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                      140.0                156.0                 30.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "411                      NaN                  NaN                  NaN  \n",
            "412                      NaN                  NaN                  NaN  \n",
            "413                      NaN                  NaN                  NaN  \n",
            "414                      NaN                  NaN                  NaN  \n",
            "415                      NaN                  NaN                  NaN  \n",
            "\n",
            "[416 rows x 8 columns]\n",
            "Printing sheet: Q10 Docs\n",
            "     Sr. No.  Topic Id   Doc Id  Doc Relevancy  Irrelevant Doc  \\\n",
            "0          1        10    96856              1           146.0   \n",
            "1          2        10   716627              1             NaN   \n",
            "2          3        10  2786658              2             NaN   \n",
            "3          4        10  2570516              2             NaN   \n",
            "4          5        10   630617              2             NaN   \n",
            "..       ...       ...      ...            ...             ...   \n",
            "357      358        10  2804533              2             NaN   \n",
            "358      359        10  1768316              3             NaN   \n",
            "359      360        10  2117093              4             NaN   \n",
            "360      361        10  2168003              1             NaN   \n",
            "361      362        10   729720              1             NaN   \n",
            "\n",
            "     Marginally relevant Doc  Fairly relevant Doc  Highly relevant Doc  \n",
            "0                      150.0                 36.0                 30.0  \n",
            "1                        NaN                  NaN                  NaN  \n",
            "2                        NaN                  NaN                  NaN  \n",
            "3                        NaN                  NaN                  NaN  \n",
            "4                        NaN                  NaN                  NaN  \n",
            "..                       ...                  ...                  ...  \n",
            "357                      NaN                  NaN                  NaN  \n",
            "358                      NaN                  NaN                  NaN  \n",
            "359                      NaN                  NaN                  NaN  \n",
            "360                      NaN                  NaN                  NaN  \n",
            "361                      NaN                  NaN                  NaN  \n",
            "\n",
            "[362 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "doc = docx.Document('queries.docx')\n",
        "table = doc.tables[0]  # assuming the table is the first one in the document\n",
        "\n",
        "# Create a prettytable with the appropriate column headers\n",
        "ptable = PrettyTable(['Sr.#', 'Domains', 'Topics', 'Unwanaat'])\n",
        "\n",
        "# Iterate through the rows of the table and add them to the prettytable\n",
        "for i, row in enumerate(table.rows):\n",
        "    if i == 0:  # skip the header row\n",
        "        continue\n",
        "    sr_no, domains, topics, unwanaat = [cell.text for cell in row.cells]\n",
        "    ptable.add_row([sr_no, domains, topics, unwanaat])\n",
        "\n",
        "# Print the prettytable\n",
        "print(ptable)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSen0ZFkhZ6L",
        "outputId": "d5b6d28f-ee5e-43f0-91d4-dc36dca873bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+-----------------------------------------------------+--------------------------------------------------------+\n",
            "| Sr.# |     Domains      |                        Topics                       |                        Unwanaat                        |\n",
            "+------+------------------+-----------------------------------------------------+--------------------------------------------------------+\n",
            "|  1   | Law & Government |               The criminal was hanged               |                مجرم کو پھانسی دے دی گئی                |\n",
            "|  2   | Law & Government |       Protest marches, procession, and rallies      |            احتجاجی  دھرنے, جلوس اور ریلیاں             |\n",
            "|  3   | Law & Government |          Anti-smoking actions and awareness         |        انسداد تمباکو نوشی کے اقدامات اور آگاہی         |\n",
            "|  4   | Law & Government |              Lawyers strike and rallies             |               وکلاء کی ہڑتال اور ریلیاں                |\n",
            "|  5   | Law & Government |               Secret agencies and spy               |                خفیہ ایجنسیاں اور جاسوس                 |\n",
            "|  6   | Law & Government |              Accountability court cases             |                 احتساب عدالت کے مقدمات                 |\n",
            "|  7   |   Human Rights   | Violence against children and abduction of children |               بچوں کے خلاف تشدد اور اغوا               |\n",
            "|  8   |   Human Rights   |                  Violence on Women,                 |          عورتوں پر ظلم و تشدد , قتل اور اغوا           |\n",
            "|  9   |   Human Rights   |                   Human Smuggling                   | انسانوں کی اسمگلنگ اور انسداد انسانی سمگلنگ کے اقدامات |\n",
            "|  10  |   Human Rights   |             Workers' prosperity measures            |            مزدور طبقے کی خوشحالی کے اقدامات            |\n",
            "+------+------------------+-----------------------------------------------------+--------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import docx\n",
        "from collections import defaultdict\n",
        "from operator import itemgetter\n",
        "import math\n",
        "\n",
        "# Step 1: Read the queries.docx file to extract the queries.\n",
        "def extract_queries(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    queries = []\n",
        "    for p in doc.paragraphs:\n",
        "        queries.append(p.text)\n",
        "    return queries\n",
        "\n",
        "# Step 2: Read the qrels.xlsx file to extract the relevance grades.\n",
        "def extract_qrels(file_path):\n",
        "    qrels = defaultdict(lambda: defaultdict(int))\n",
        "    df = pd.read_excel(file_path)\n",
        "    for _, row in df.iterrows():\n",
        "        qrels[row['topic']][row['doc']] = row['grade']\n",
        "    return qrels\n",
        "\n",
        "# Step 3: Implement the chosen scoring function (TF-IDF).\n",
        "def tfidf_score(query, doc):\n",
        "    score = 0\n",
        "    for term in query:\n",
        "        if term in doc:\n",
        "            tf = doc.count(term)\n",
        "            idf = log10(N / df[term])\n",
        "            score += tf * idf\n",
        "    return score\n",
        "\n",
        "# Step 4: For each query, compute the scores for all documents.\n",
        "def compute_scores(queries, docs, qrels, score_fn):\n",
        "    scores = defaultdict(list)\n",
        "    for topic, query in enumerate(queries, start=1):\n",
        "        for doc_id, doc in docs.items():\n",
        "            doc_score = score_fn(query, doc)\n",
        "            relevance = qrels[topic][doc_id]\n",
        "            scores[topic].append((doc_id, relevance, doc_score))\n",
        "        scores[topic] = sorted(scores[topic], key=itemgetter(2), reverse=True)\n",
        "    return scores\n",
        "\n",
        "# Step 5: Print the ranked list of documents.\n",
        "def print_ranked_list(scores, run_name):\n",
        "    for topic, topic_scores in scores.items():\n",
        "        for rank, (doc_id, relevance, score) in enumerate(topic_scores, start=1):\n",
        "            print(f\"{topic} {doc_id} {rank} {score:.2f} {run_name}\")\n",
        "\n",
        "# Step 6: Parse command-line arguments.\n",
        "parser = argparse.ArgumentParser(description='Rank documents for queries using a scoring function')\n",
        "parser.add_argument('--score', type=str, help='the name of the scoring function to use', default='tfidf')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Step 7: Run the program.\n",
        "if __name__ == '__main__':\n",
        "    file_path = 'queries.docx'\n",
        "    queries = extract_queries(file_path)\n",
        "    file_path = 'qrels.xlsx'\n",
        "    qrels = extract_qrels(file_path)\n",
        "    docs = {}  # assuming you have a dictionary of document IDs and their content\n",
        "    N = len(docs)\n",
        "    df = defaultdict(int)\n",
        "    for doc in docs.values():\n",
        "        for term in set(doc):\n",
        "            df[term] += 1\n",
        "    if args.score == 'tfidf':\n",
        "        score_fn = tfidf_score\n",
        "    scores = compute_scores(queries, docs, qrels, score_fn)\n",
        "    run_name = 'run1'\n",
        "    print_ranked_list(scores, run_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "Xqj0gEqRhhqr",
        "outputId": "bc10cb12-dfc0-4a97-c3c9-3fdf896cfc43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--score SCORE]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-15e5d86c-a855-4664-bd75-adcd1a2fe893.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "\n",
        "# Load the stop words\n",
        "with open('Urdu stopwords.txt', 'r', encoding='utf-8') as f:\n",
        "    stop_words = set([line.strip() for line in f])\n",
        "\n",
        "# Load the document index\n",
        "with open('doc_index.txt', 'r', encoding='utf-8') as f:\n",
        "    doc_index = eval(f.read())\n",
        "\n",
        "# Load the document ids\n",
        "with open('docids.txt', 'r', encoding='utf-8') as f:\n",
        "    docids = eval(f.read())\n",
        "\n",
        "# Load the term index\n",
        "with open('term_index.txt', 'r', encoding='utf-8') as f:\n",
        "    term_index = eval(f.read())\n",
        "\n",
        "# Load the term information\n",
        "with open('term_info.txt', 'r', encoding='utf-8') as f:\n",
        "    term_info = eval(f.read())\n",
        "\n",
        "# Load the term ids\n",
        "with open('termids.txt', 'r', encoding='utf-8') as f:\n",
        "    termids = eval(f.read())\n",
        "\n",
        "# Define the scoring function\n",
        "def score_tf_idf(query_terms):\n",
        "    scores = {}\n",
        "    N = len(docids)\n",
        "    for term in query_terms:\n",
        "        if term in stop_words:\n",
        "            continue\n",
        "        if term not in term_index:\n",
        "            continue\n",
        "        df = len(term_index[term])\n",
        "        idf = math.log10(N / df)\n",
        "        for doc_id, tf in term_index[term].items():\n",
        "            if doc_id not in scores:\n",
        "                scores[doc_id] = 0\n",
        "            scores[doc_id] += tf * idf\n",
        "    return scores\n",
        "\n",
        "# Process the query\n",
        "def process_query(query):\n",
        "    # Split the query into tokens\n",
        "    query_terms = query.split()\n",
        "    # Apply stop-wording\n",
        "    query_terms = [term for term in query_terms if term not in stop_words]\n",
        "    # Apply stemming\n",
        "    query_terms = [termids.get(term, term) for term in query_terms]\n",
        "    return query_terms\n",
        "\n",
        "# Parse command line arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--score', type=str, default='TF-IDF')\n",
        "parser.add_argument('--query', type=str, required=True)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Process the query\n",
        "query_terms = process_query(args.query)\n",
        "\n",
        "# Calculate the scores using the selected scoring function\n",
        "if args.score == 'TF-IDF':\n",
        "    scores = score_tf_idf(query_terms)\n",
        "else:\n",
        "    print('Invalid scoring function')\n",
        "    exit()\n",
        "\n",
        "# Rank the documents\n",
        "ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the results\n",
        "for i, (doc_id, score) in enumerate(ranked_docs):\n",
        "    print(f'{docids[doc_id]} {doc_id} {i+1} {score} {args.score}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "6GflfeFnr0Cu",
        "outputId": "a74bd845-41be-4cb8-e27d-0bbe5d67b10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-70-a315eda28358>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    doc_index = eval(f.read())\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1\t1\t1\u001b[0m\n\u001b[0m     \t^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('doc_index.txt', 'r', encoding='utf-8') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "M-CFwzd1uVPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}